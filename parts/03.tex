\part{Praxisteil}

\section{Versuchsaufbau}
Der Optimierungsversuch kann leider nicht auf dem Livesystem durchgeführt werden, da dass die Stabilität beeinträchtigen würde. Deswegen wurde ein Entwicklungssystem genutzt, um die verschiedenen Methoden zu analysieren. Das Entwicklungssystem ist eine neuere Anschaffung der pludoni GmbH und dadurch, im Vergleich zum Livesystem, leistungsfähiger. Dadurch wird sich das Endergebnis, wenn es nach dem Abschluss der Arbeit auf das Livesystem übertragen wurde, in seinen Kenndaten unterscheiden. Trotz alledem werden die Verbesserungen relativ zum Testsystem proportional ausfallen. 

\begin{itemize}
 \item Testplattform: pludoni Server eq4
  \item Prozessor: Intel® Core™ i7-920
  \item Arbeitsspeicher: 8 GB DDR3 RAM
  \item Festplatten: 2 x 750 GB SATA-II HDD (Software-RAID 1)
  \item Netzwerkverbindung: 100Mbit
  \item Server Software: Apache/2.2.16
  \item Server Hostname: itsax.it-jobs-und-stellen.de
  \item Server Port: 80
\end{itemize}

\section{Testverfahren}
Das Testen von Web-Performance hat sich in den letzten Jahren stetig weiterentwickelt. Eine Zeit lang galten sogenannte Backbonetests als das Non-Plus-Ultra. Nachdem aber immer deutlicher wurde, dass die tatsächlichen Ergebnisse sich stark von den Erwartungen unterscheiden, suchte man nach Alternativen. Der verlässlichste Test wird natürlich direkt beim Kunden beziehungsweise der Zielgruppe durchgeführt. Da es oft schwierig ist diese Tests durchzuführen, sollte zumindest mit ähnlichen Vorraussetzungen getestet werden.
\subsection{Servertest}
Die rein serverseitigen Optimierungen wurden mit ``ab'' getestet. ``ab'', das für apache bench steht, wurde entwickelt, um Apache Server zu testen. Es analysiert, wie lange ein Server braucht, um Webseiten auszuliefern und wieviele Anfragen er pro Sekunde befriedigen kann, ohne das es zu Ausfällen kommt. ``ab'' führt 50 Anfragen hintereinander auf die Ressource aus und berechnet dann anhand der Ergebnisse die durchschnittliche Antwortzeit. Der Kommandozeilenbefehl, der zum Testen genutzt wurde, war denkbar einfach:
\begin{lstlisting}[language=bash,label=ab mit Paramtern,caption=ab mit Parametern]
ab -n 50 -c 1 http://itsax.it-jobs-und-stellen.de/
\end{lstlisting}

\subsection{Browsertest}
Browsertests sind eine sehr komplizierte Angelegenheit, da es zum einen sehr viele verschiedene Browser gibt und zum anderen diese Browser in den verschiedensten Umgebungen ausgeführt werden. Ein Netbook zum Beispiel wird eine Webseite immer langsamer darstellen als ein Desktop-PC der neuesten Generation. Man kann natürlich am eigenen Computer Tests durchführen, dafür gibt es aber auch Programme, wie Firebug und YSlow, die auch Verbesserungsvorschläge liefern können. Da aber möglichst objektiv getestet werden soll, ist es von Vorteil, eine standardisierte und automatisierte Umgebung zu nutzen. Für diesen Zweck bietet sich WebPagetest an. Es wurde ursprünglich für den internen Gebrauch bei AOL entwickelt, steht aber seit 2008 unter der BSD Lizenz und kann somit frei genutzt werden. Es ist eine umfassende Testsuite für Webseiten. Besonders die verschiedenen Analysen, die automatisch durchgeführt werden, sind positiv hervorzuheben. Das Programm ist in der Lage, die Webseite nach den Google Page Speed Kriterien zu bewerten und ausführliche Inhaltsanalysen, aufgeschlüsselt nach MIME Typen, zu liefern. Zusätzlich wird ein sehr hilfreiches Wasserfalldiagramm erzeugt und für jedes Element der Webseite ein Performance Review erstellt. Darin werden unter anderem Kriterien wie Kompression und Caching betrachtet. Als besonderes Feature gibt es noch einen Videovergleich, in dem man Webseiten gegeneinander antreten lassen kann. Man kann WebpageTest sowohl selbst installieren als auch externe Anbieter nutzen. In diesem Fall wird www.webpagetest.org mit folgenden Paramtern genutzt:
\begin{itemize}
  \item 10 konsekutive Tests (maximum) 
  \item Standort des Clients: Frankfurt a. M.
  \item Browser: IE9
  \item Connection: DSL 1,5 Mbps / 50ms RTT
  \item Only First View
\end{itemize}
\subsection{Andere Analyse Verfahren}
Profiling und Debugging auf Codeebene wurden nur sporadisch eingesetzt, da sie den Anwendungsfall Webseite nur eingeschränkt betreffen. Durch Caching kann der Server zum großen Teil vollständig entlastet werden und die Codeausführungszeit kann eliminiert werden. Bei Anwendungsfällen, die kein Caching erlauben, wie zum Beispiel eine Suche oder eine Applikation, ist es sehr hilfreich diese Tools zu verwenden. Sie ersparen viel Arbeit, da die Problemstellen schnell ausgemacht werden können. Von Hand diese Arbeiten durchzuführen, ist in kleinen Projekten sehr mühsam und in großen Projekten fast unmöglich. Jeder Entwickler sollte solche Programme in seinem Repertoire haben.

\section{Ausgangszustand}
Sreeram Ramachandran, ein Software Ingenieur der Firma Google, hat eine Analyse über 4.2 Milliarden Seiten veröffentlicht. Diese ist im Rahmen der Initiative ``Let's make the web faster'' entstanden und zeigt häufige Fehlerquellen und ungenutztes Potential auf. Die durchschnittliche Webseite hat laut Ramachandran 320 kB Größe, 44 verschiedene Ressourcen und es werden nur 66\% der komprimierbaren Inhalte tatsächlich komprimiert. 
Itsax.de hat 106 Ressourcen und 444 kB an Daten. Schon anhand dieser zwei Zahlen lässt sich eine vergleichsweise schwache Leistung vorhersehen. Besonders die Anzahl an verschiedenen Ressourcen deutet auf Missstände hin, da Parallelisierung von Zugriffen nur bis zu einem bestimmten Grad möglich ist. Die Time to First Byte(TtFB) von 674ms bezeichnet die Zeit, die vergeht bis der Webbrowser die ersten Daten empfängt, dass bedeutet aber noch nicht, dass der Nutzer schon Inhalte präsentiert bekommt. Die Inhalte werden erst angezeigt, nachdem die Time to Start Render(TtSR) vergangen ist. Der Nutzer muss demnach ungefähr zwei Sekunden warten, bis die Webseite im Browser anfängt sich aufzubauen. Die Load Time(LT) bezeichnet dann die Zeit die vergeht, bis die Seite komplett dargestellt wird und der Benutzer sie ohne Einschränkungen bedienen kann. Es können aber auch nach der LT noch Inhalte nachgeladen werden, wie zum Beispiel gestreamte Videos oder andere asynchrone Inhalte. Diese nachgeladenen Inhalte wirken sich aber nicht mehr negativ auf die User Experience aus, solange sie im Rahmen bleiben und nicht wichtige Teile der Webseite, wie zum Beispiel das Hauptmenü, noch per Flash geladen werden müssen. Die Anzahl der DOM Elemente bezeichnet alle vom Browser zu verarbeitenden Objekte und ist ein Indikator für die Komplexität der Webseite. Je mehr Elemente also vorhanden sind, desto länger muss der Browser die Positionierung und Darstellung berechnen. Mit 855 DOM Elementen gehört ITsax.de schon zu den komplexeren Seite, was zu einem Großteil Drupal anzurechnen ist, welches durch seine gute Konfigurierbarkeit Einbußen in    Die Inhalte auf der Seite Itsax.de sind in Abbildung ? dargestellt, einmal im Bezug auf Größe und einmal aufgeschlüsselt nach der Anzahl der benötigten Requests, um die Inhalte vom Server anzufordern. Die folgenden Ergebnisse wurden mit dem Analysetool webpagetest.org ermittelt. Leicht zu erkennen ist, dass der HTML Anteil, sowohl bei der Größe als auch bei Anzahl der HTTP Anfragen, eine untergeordnete Rolle spielt. Nicht vergessen werden darf aber die Zeit, die der Server benötigt, um den HTTP Code zu generieren. Dies kann man sehr gut in einem Wasserfalldiagramm erkennen, indem der Start des Ladevorgangs hervorgehoben wurde. Bevor der Initiale GET Request abgeschlossen ist, weiß der Browser noch nicht, welche Ressourcen er laden muss und es können noch keine anderen Aktionen ausgeführt werden.
\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{material/start_waterfall_edited.png}
  \caption{Wasserfalldiagramm: Ausgangszustand}
  \label{fig:startwaterfall}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{material/start_request_pie.png}
  \caption{HTTP Anfragen}
  \label{fig:startrequest}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{material/start_byte_pie.png}
  \caption{Größe der Inhalte}
  \label{fig:startbyte}
\end{figure}

Load Time: 3.728
Time to First Byte: 0.674s 	
Time to Start Render: 2.002s
\#DOM Elements: 855 	

\begin{tabbing}
Request \quad\= blablabla \quad\= \kill
\textbf{Typ} 	 \> \textbf{Anzahl} \\
text/css	 \>	24 	\\
image/gif	 \>	23 	\\
image/jpeg	 \>	22 	\\
javascript	 \>	20 	\\ 
image/png	 \>	15 	\\
text/html	 \>	2 	\\
\end{tabbing}

\begin{tabbing}
Request \quad\= blablabla \quad\= \kill
\textbf{Typ} 	 \> \textbf{Byte} \\
image/jpeg	\>142251\\
image/png	\>101428\\
javascript	\>84758\\
image/gif	\>73378\\
text/css	\>30222\\
text/html	\>29917\\

\end{tabbing}
%http://code.google.com/intl/de/speed/articles/web-metrics.html
\begin{lstlisting}[language=bash,label=Ausgabe von ab,caption=Ausgabe von ab]
Server Software:        Apache/2.2.16
Server Hostname:        itsax.it-jobs-und-stellen.deDocument Path:          /
Document Length:        65218 bytes

Concurrency Level:      1
Time taken for tests:   18.182 seconds
Complete requests:      50

Write errors:           0
Total transferred:      3266726 bytes
HTML transferred:       3239226 bytes
Requests per second:    2.75 [#/sec] (mean)
Time per request:       363.640 [ms] (mean)
Time per request:       363.640 [ms] (mean, across all concurrent requests)
Transfer rate:          175.46 [kBytes/sec] received
\end{lstlisting}




\section{Implementierung und Test der einzelnen Methoden}
Im folgenden Abschnitt werden die eingesetzten Methoden dargestellt und ihre Auswirkungen auf die Webperformance der Seite itsax.de aufgezeigt. 
Dabei werden alle Methoden, die vom Autor, nach der Analyse des Ausgangszustands, für möglicherweise sinnvoll befunden wurden, getestet. Die Methode wird jeweils mit dem Startwert verglichen und der Aufwand eingeschätzt.
\subsection{APC} Als allererste Maßnahme wurde ein PHP Opcode Cache ausprobiert. Opcode Caches speichern die Ergebnisse vorangegangener Operationen und können so beim nächsten Aufruf direkt die Ergebnisse ausliefern, wenn die Anfrage gleichgeblieben ist. Solche Systeme sind immer zu empfehlen, da sie normalerweise keinerlei Probleme bereiten und alle PHP Anfragen beschleunigen. Aktuell gibt es mehrere verschiedene sogenannte PHP Beschleuniger. Unter ihnen nimmt aber APC eine führende Position ein. Hauptsächlich weil es der zuverlässigste unter den nicht-kommerziellen Caches ist. Ab PHP Version 5.4 wird er in PHP schon enthalten sein. Die Installation von APC ist auf einigermaßen aktuellen Linux Servern auch denkbar einfach. Das PHP Pear Framework, was oft schon installiert ist, erlaubt mit einigen wenigen Befehlen einen funktionierenden Opcode Cache zu installieren. 
\begin{lstlisting}[language=bash,label=Installation von APC,caption=Installation von APC]
apt-get install php-pear
pecl install apc
\end{lstlisting}
Nach der Installation wurde die Antwortzeit des Apacheservers gemessen und es kam zu folgendem Ergebnis. Statt 363 ms wurden nur noch 233 ms benötigt, eine Verbesserung um 36\% und es können 4,28 Anfragen je Sekunde bearbeitet werden gegenüber vorher nur 2,75. Alles unter der Prämisse, dass es keine multiplen Anfragen gibt. 
\begin{lstlisting}[language=bash,label=Ausgabe von ab,caption=Ausgabe von ab]
Server Software:        Apache/2.2.16
Server Hostname:        itsax.it-jobs-und-stellen.de
Server Port:            80

Document Path:          /
Document Length:        64842 bytes

Concurrency Level:      1
Time taken for tests:   11.681 seconds
Complete requests:      50

Write errors:           0
Total transferred:      3261730 bytes
HTML transferred:       3234230 bytes
Requests per second:    4.28 [\#/sec] (mean)
Time per request:       233.620 [ms] (mean)
Time per request:       233.620 [ms] (mean, across all concurrent requests)
Transfer rate:          272.69 [kBytes/sec] received
\end{lstlisting}
\subsection{Drupal Cache}
\begin{lstlisting}[language=bash,label=Ausgabe von ab,caption=Ausgabe von ab]
Server Software:        Apache/2.2.16
Server Hostname:        itsax.it-jobs-und-stellen.de
Server Port:            80

Document Path:          /
Document Length:        65005 bytes

Concurrency Level:      1
Time taken for tests:   2.082 seconds
Complete requests:      50
Failed requests:        0
Write errors:           0
Total transferred:      3276900 bytes
HTML transferred:       3250250 bytes
Requests per second:    24.02 [\#/sec] (mean)
Time per request:       41.638 [ms] (mean)
Time per request:       41.638 [ms] (mean, across all concurrent requests)
Transfer rate:          1537.09 [kBytes/sec] received
\end{lstlisting}

%http://www.webpagetest.org/result/110809_BH_198S4/


\subsection{JS Aggregation und Minifizierung mit dem Javascript Aggregation Modul}
Aggregation und Minifizierung sind Verfahren, die in der Webperformance Optimierung häufig eingesetzt werden. Für das Drupal 5 System gibt es fertige Module die diese Aufgabe übernehmen. Das Modul interveniert innerhalb des Drupal Kerns und ersetzt die Javascripts, die vorher direkt so ausgegeben wurden wie sie die Module lieferten, durch eine einzelne, das heisst aggregierte Version, die optional minifiziert werden kann. Diese Minifizierung wird natürlich genutzt und spart einige kB. Minifizierung wird ermöglicht durch die Nutzung von JSmin https://github.com/rgrove/jsmin-php/. JSmin ist ein Filter der unter anderem Kommentare entfernt und mehrere Leerzeichen zu einem zusammenfasst. Durch die Nutzung dieser Methode konnten 11 HTTP Abfragen und 11 kB an Datenvolumen eingespart werden.
Load Time: 3.658s
Time to First Byte: 0.595s
Time to Start Render: 1.915s
\#DOM Elements: 844 	
\#Requests: 95 %!
Bytes In: 432 kB
%http://www.webpagetest.org/result/110809_Z1_198TK/

\subsection{CSS Aggregation und Komprimierung}
Analog zur Javascript Optimierung kann man auch die CSS Aggregation betrachten. Dabei werden durch Zusammenfassung der einzelnen CSS Dateien HTTP Abfragen eingespart. Wie man an der gesunkenen Anzahl der Requests sehen kann, wurden 21 Abfragen nur durch Zusammenfügen der einzelnen CSS Dateien zu einer einzigen eingespart. Durch die Bereinigung beziehungsweise Komprimierung werden dabei außerdem 17 kB an überflüssigen Zeichen entfernt. 
Load Time: 3.577s
Time to First Byte: 0.649s
Time to Start Render: 1.577s
\#DOM Elements: 834 	
\#Requests: 85 %!
Bytes In: 427 kB
%http://www.webpagetest.org/result/110809_XB_198VE/
\subsection{Drupal Boost Module} Das Boost Modul ist ein Cache für statische Seiten, er funktioniert aber nur für Gastnutzer der Webseite. ITsax.de wird aber fast ausschließlich von Gastnutzern besucht, da nur Partner und Administratoren einen erweiterten Zugang benötigen. Durch diese guten Vorraussetzungen kann das Boost Modul komplette Seiten cachen. Dabei ist HTML, XML, Ajax, CSS und Javascript caching und gzipping möglich. Die genutzen Techniken sind sehr performant und bauen auf einem Dateisystemcache auf, das bedeutet jede Seite wird komplett auf der Festplatte abgelegt. Alle Arten von serverseitigen Prozessen werden so nach dem initialem Seitenaufruf, der das Caching auslöst, nicht mehr durchlaufen. Dies sieht man sehr gut an der Time to First Byte. Von den 172 ms werden ca. 50ms für den DNS Lookup benötigt und 70ms für die erste HTTP Verbindung. Der Server braucht demnach nur ca. 50 ms um die Seite auszuliefern. Dies hat natürlich einen sehr Positiven Einfluss auf die Gesamtperformance und macht sich besonders beim Endnutzer bemerkbar, da die Seite spürbar schneller anfängt sich im Browser aufzubauen. 
Load Time: 3.233s
Time to First Byte: 0.172s %!
Time to Start Render: 1.473s
\#DOM Elements: 856 	
\#Requests: 106 %!
Bytes In: 444 kB
%http://www.webpagetest.org/result/110809_AP_198XJ/

\subsection{Bildkompression mit jpegoptim und OptiPNG}
Bilder und Grafiken bieten oft großen Optimierungsspielraum. Zum einen erfolgt dies durch die richtige Auswahl der Dateiformate und zum anderen durch Komprimierung der Bilder. Da es auf www.itsax.de nicht nur statische Inhalte gibt, sondern auch durch Communitymitglieder und Communitymanager eingestellte Inhalte verwaltet werden müssen, sollte eine nachträgliche Qualitätsoptimierung der hochgeladenen Bilder durchgeführt werden. Um dies umzusetzen, sind die Programme OptiPNG und jpegOptim zu empfehlen. Da es sich bei beiden Programmen um Kommandozeilenprogramme handelt, kann man ihre Anwendung leicht automatisieren. Mit dem Linuxbefehl find, der praktischerweise eine Möglichkeit, Befehle auszuführen, besitzt, kann man direkt die entsprechenden Dateien an die Optimierer übergeben. Diese Aktionen können dann über einen Cronjob periodisch jede Nacht ausgeführt werden. 

\subsubsection{Verlustfreie Kompression} Technisch wird eine verlustfreie Kompression durch Verfahren wie die Huffman-Codierung oder die Lauflängenkodierung umgesetzt. Im Fall des PNG Formats wird die Komprimierungsmethode Deflate genutzt. Außerdem werden Vorfilter, in Form von Prädikativer Kodierung, eingesetzt. Diese berechnen die wahrscheinlichen Farbwerte und es müssen nur die Abweichungen gespeichert werden. 
%TODO quelle

Die Befehle sehen dann wie folgt aus:

\begin{lstlisting}[language=bash,label=Optimieren mit find,caption=Optimieren mit find]
find . -name "*.png" -exec optipng -o7 {} \;
find . -name "*.jpg" -exec jpegoptim {} \;
\end{lstlisting}
Auf jeden Fall sollte eine verlustfreie Komprimierung durchgeführt werden, da die Bilder in diesem Fall nur an Dateigröße verlieren und die Bildqualität unberührt bleibt. 
%%http://optipng.sourceforge.net/
%%http://www.kokkonen.net/tjko/projects.html
Load Time: 3.640
Time to First Byte: 0.636s %!
Time to Start Render: 1.894s
\#DOM Elements: 855 	
\#Requests: 106 %!
Bytes In: 429 kB % 15kb gespart
%http://www.webpagetest.org/result/110809_C4_1999C/
\subsubsection{Verlustbehaftete Kompression} Da im Web kein besonders hoher Detailgrad gewährleistet werden muss, ist auch eine Kompression erwünscht die auf Kosten der Bildqualität die Bildgröße verringert. 

find . -name "*.png" -exec optipng -o7 {} \;
find . -name "*.jpg" -exec jpegoptim {} \;

Load Time: 3.255s
Time to First Byte: 0.669s %!
Time to Start Render: 1.908s
\#DOM Elements: 855 	
\#Requests: 106 %!
Bytes In: 371 kB % 73kb gespart
%http://www.webpagetest.org/result/110809_15_199GY/

\subsection{Drupal 5 Fehler bei umgefärbten Themes}
Das Framework Drupal 5 benutzt Themes zur Gestaltung der Oberfläche. Um diese farblich anpassen zu können, wurde das Color Modul installiert, welches Themeveränderungen ermöglicht. Aufgrund der Tatsache, dass das Theme nur kopiert wird und im Anschluss die Farben geändert werden, entstehen bei diesem Vorgang unnötige Duplikate, die beim Laden der Seite mitgeschleppt werden. Um diese zu entfernen, wird einfach das Standardtheme durch das Modifizierte ersetzt. Dafür müssen  nur noch einige Pfade in der style.css angepasst werden und man spart in dem Fall von itsax.de 4 kB, was immerhin ca 1\% der übertragenen Datenmenge ausmacht. 
händisch gemergte styles
style auf standard setzen
vorher die images und das geänderte stylesheet kopieren
Load Time: 3.626s
Time to First Byte: 0.629s %!
Time to Start Render: 1.890s
\#DOM Elements: 854 	
\#Requests: 104 %!
Bytes In: 440 kB % 4kb gespart
%http://www.webpagetest.org/result/110809_SZ_19APH/

\subsection{Theme Bilder Spriten}
Spriting wurde ursprünglich in der Videospielentwicklung verwendet, um Bilder in den Grafikspeicher zu laden. In der Webentwicklung ist es eine effektive Technik, um Bilder ohne mehrfachen Overhead zu laden. Beim Spriting wird aus vielen einzelnen Bildern ein einziges Bild erstellt, dass anstelle der vielen Bilder geladen wird. Um die Bilder dann noch einzeln Anzeigen zu können, werden CSS Befehle genutzt, die es ermöglichen, die Größe und die Position eines Bildausschnittes anzuzeigen. 
Load Time: 3.707
Time to First Byte: 0.669s %!
Time to Start Render: 1.968s
\#DOM Elements: 855 	
\#Requests: 103 
Bytes In: 443 kB 
%http://www.webpagetest.org/result/110810_FZ_19GBA/

\subsection{Module von Startseite entfernen}
Um zu überprüfen, welchen Einfluss verschiedene, im Netzwerkgraphen auffällige, Module auf die Gesamtperformance haben, werden sie testweise komplett deaktiviert. So kann man entscheiden, bei welchen Modulen zusätzlicher Programmieraufwand lohnenswert ist.
\subsubsection{Partnerslideshow} Das Deaktivieren der Partnerslideshow hat gravierenden Einfluss auf die Gesamtperformance. Das Modul lädt im Aktiven zustand alle Bilder die es benötigt und bremst damit den gesamten Ladevorgang aus. Die Load Time verringert sich um 27,7\%. Der größte Teil des Geschwindigkeitszuwachses ist der Verringerung der Übertragungsmenge zuzuschreiben. 
Load Time: 2.695s
Time to First Byte: 0.636s %!
Time to Start Render: 1.838s
\#DOM Elements: 790
\#Requests: 80
Bytes In: 276 kB

\subsubsection{Facebook Widget} In diesem Fall ist der Einfluss auf die Performance geringer, aber mit einer Verbesserung von 6,7\% auf jeden Fall vorhanden. Das Widget besteht aus neun kleinen Fotos und dem Facebookrahmen. Den größten Effekt hat hier die Verringerung der Anzahl der HTTP Anfragen. So können wichtigere Elemente schneller geladen werden.
Load Time: 3.478s
Time to First Byte: 0.668s %!
Time to Start Render: 1.966s
\#DOM Elements: 779 	
\#Requests: 95 
Bytes In: 406 kB 

\subsubsection{Jobleiste deaktiviert} Die Jobleiste hat einen geringfügig größeren Einfluss auf die Ladezeiten als das Facebook Widget. Die Ursache ist wahrscheinlich in den zusätzlichen Bibliotheken zu suchen, die zu diesem Modul gehören. Darunter sind Ajax Bibliotheken und anderer ThirdParty Code. 
Load Time: 3.367s
Time to First Byte: 0.617s %!
Time to Start Render: 1.709s
\#DOM Elements: 822 	
\#Requests: 94 
Bytes In: 411 kB

\subsection{Umprogrammierung der Module}
Um die langsamen Module weiterhin nutzen zu können, muss eine Lösung gefunden werden, die es ermöglicht, Inhalte nachzuladen, nachdem die Seite komplett geladen wurde. Um diese Asynchronität zu erreichen, wird mit Hilfe eines Timeout-Befehls, das Laden der nicht priorisierten Inhalte verzögert. Programmiert wird dieses Verhalten mit Javascript, genauer JQuery. Besonders betrachtet werden muss dabei, ob eine vorhandene Dynamik erhalten bleiben soll. Dazu gehören zum Beispiel zufällige Elemente oder Elemente mit besonders häufigen Aktualisierungen.

\subsubsection{Partnerslideshow} Die Partnerslideshow ist eher ein kosmetisches Element der Startseite und für den Nutzer nicht notwendig. Daher kann es so Programmiert werden, dass beim Laden nur ein leeres DIV übergeben wird. Dann wird ein Timeout gestartet und bei der Aktivierung des Timeoutevents wird das DIV mit den Slideshowelementen gefüllt, und die Slideshow gestartet. Der Code wurde dahingehend angepasst, dass anstatt den HTML Code in der Seite einzufügen er in eine Javascript Variable geschrieben wird. Dadurch ist es möglich über eine DOM Manipulation den HTML Code später einzufügen. Das stellt sich dann wie im folgenden Codeaussschnitt dar.
\begin{lstlisting}[language=php,label=Javascript - Slideshow,caption=Javascript - Slideshow]
drupal_add_js('
  if (Drupal.jsEnabled) {
  $(document).ready(function() {
     window.setTimeout(delayed_partnerbox,500);
  });
  function delayed_partnerbox(){
	$("#projects").parent().html(partner_box_load);
	$(".slideshow").cycle({
	    delay:  200 ,
	    height: "80px",
	    width: "180px",
	    containerResize: 0,
	    fit: 1,
	    fx: "fade"
	});
  }
}', 'inline')
\end{lstlisting}
An das window Objekt, was die Webseite verkörpert, wird in Zeile 3 ein Timer gehängt. Der Timer löst nach 500 ms einen Callback aus, durch den die Funktion delayed\_partnerbox ausgeführt wird. Durch diese Verlagerung des Inhalts braucht die Webseite nur noch 2.749s zum laden. Bis die Webseite die nachgeladene Slideshow anzeigt vergehen 3.794s. Die Webseite ist also eher benutzbar und später komplett geladen. In Testverfahren wie zum Beispiel für das Google Ranking wird nur betrachtet wann die Seite das Onload Event auslöst. Somit wurde die Seite für Google und den Nutzer schneller. Der Performancegewinn geht wie in der vorherigen Analyse festgestellt aus der verringerten Datenmenge hervor. Natürlich spielt auch die Reduzierung der HTTP Anfragen eine Rolle. Die Datenmenge wurde auf 284 kB verringert und es wurden statt 107 nur 82 HTTP Anfragen benötigt.
%Time to First Byte: 0.626s %!
%Time to Start Render: 1.842s
%\#DOM Elements: 855 	
%\#Requests: 82 (107)
%Bytes In: 284 kB (395)
%http://www.webpagetest.org/result/110810_3Z_19MRE/

\subsubsection{Facebook Widget} Das Facebook Widget wurde durch Entwickler der pludoni GmbH so programmiert, dass es das von Facebook zur Verfügung gestellte Widget cacht und dann auf der Webseite darstellt. 
Load Time: 3.411s (4.258)
Time to First Byte: 0.576s %!
Time to Start Render: 1.848s
\#DOM Elements: 857
\#Requests: 95 (106)
Bytes In: 406 kB (444)
%http://www.webpagetest.org/result/110810_AJ_19N72/

\subsubsection{Partneranzeigen}
Load Time: 3.871s (5.759s)
Time to First Byte: 0.669s %!
Time to Start Render: 2.255s
\#DOM Elements: 857 
\#Requests: 105 (108)
Bytes In: 417 kB (448)
%http://www.webpagetest.org/result/110815_N6_1AWKT/1/details/


\section{Endergebnis} 
Nach der Zusammenführung aller Optimierungen wurde die Webseite erneut getestet. Das Ergebnis übertrifft alle Erwartungen. Nach nur einer Sekunde fängt der Browser an die Seite darzustellen. 2,913 Sekunden werden benötigt bis alle asynchronen Inhalte verfügbar sind. Auf 138 kB wurde die Webseitengröße komprimiert, so können auch langsame Internetverbindungen die Seite schnell laden. Von den 107 HTTP Anfragen sind nur noch 67 übrig geblieben, von denen werden aber nur 25 vor dem Onload Event benötigt. 
\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{material/end_waterfall.png}
  \caption{Wasserfalldiagramm: Optimierte Webseite}
  \label{fig:endwaterfall}
\end{figure}
%http://www.webpagetest.org/result/110825_5J_1DQHK/
\section{Vergleich}
\begin{center}
    \begin{tabular}{ | p{4cm} | p{1cm} | p{1cm} | p{6cm} |}
    \hline
    Methode & Load Time & TtSR & Kommentar \\ \hline
    \hline
    Ausgangszustand 	& 3,728s  & 2,002s & Der Ausgangszustand ohne Optimierungen. \\ \hline
    JS Aggregation 	& 3,658s  & 1,915s & Bei geringen Aufwand können hier erste Verbesserungen erzielt werden. \\ \hline
    CSS Aggregation 	& 3,577s  & 1,577s & Diese Optimierung sollte auch auf jeden Fall durchgeführt werden. \\ \hline
    Boost Modul 	& 3,233s  & 1,473s & Caching ist Pflicht! Nur in seltenen Ausnahmen ist davon abzusehen. \\ \hline
    Bildkompression verlustfrei 	& 3,640s  & 1,894s & Durch Verringerung der Datenmenge können besonders Handynutzer profitieren.  \\ \hline
    Bildkompression verlustbehaftet 	& 3,255s  & 1,908s & Zusätzliche verlustbehaftete Kompression sorgt oft für schnelleren Seitenaufbau, da große Mengen an Daten eingespart werden können.  \\ \hline
    Drupal 5 Theme Fehler 	& 3,626s  & 1,890s & Durch diese sehr spezielle Optimierung können auch 100ms gewonnen werden.  \\ \hline
    Theme Bilder gesprited 	& 3,707s  & 1,968s & Spriting ist eine gute Möglichkeit Requests zu sparen, macht sich aber leider nicht stark in der Ladezeit bemerkbar.  \\ \hline
    Partnerslideshow 	& 2,749s  & 1,842s & Sehr deutlich wird die Geschwindigkeit durch das asynchrone Nachladen von Inhalten verbessern.  \\ \hline
    Facebook Widget 	& 3,411s  & 1,848s & Besonders bei Inhalten von externen Quellen ist es nützlich die Ladezeiten abzukoppeln.  \\ \hline
    Partneranzeigen 	& 3,871s  & 2,225s & An diesem Beispiel wird deutlich das nachladen nicht unbedingt Verbesserung sein muss, trotzdem ist diese Umprogrammierung nötig um das Caching der Startseite zu ermöglichen.  \\ \hline
    \hline
    Gesamtergebnis 	& 1.354s  & 0.952s & Wenn alle Verbesserungen auf einmal aktiviert werden wird die Webseite dramatisch beschleunigt.  \\ \hline
    
    \hline
    \end{tabular}
\end{center}